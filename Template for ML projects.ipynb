{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Name of the Project*\n",
    "<i>Author : Vinitha Palani</i><br>\n",
    "<i>Date   : March 20th, 2017<i><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents <a id='Table-of-contents'></a>\n",
    "\n",
    "1. [Problem Definition](#Problem)\n",
    "\n",
    "2. [Required libraries](#Required-libraries)\n",
    "\n",
    "3. [Load data](#Load-data)\n",
    "    \n",
    "4. [Checking the data](#Checking-the-data)\n",
    "    - [Peek at your Data](#Peek)\n",
    "    - [Dimensions](#Dimensions)\n",
    "    - [Datatype of each feature](#Datatype)\n",
    "    - [Descriptive Statistics](#Descriptive-stats)\n",
    "    - [Class Distribution](#Class-dist)\n",
    "    - [Correlation](#Correlation)\n",
    "    - [Skew](#Skew)\n",
    "    - [Takeaway](#Takeaway1)\n",
    "    \n",
    "5. [Missing data](#Missing-data)\n",
    "\n",
    "6. [Exploratory Data analysis](#EDA)\n",
    "    - [Univariate Analysis](#EDA)\n",
    "    - [Takeaway](#Takeaway2)\n",
    "    - [Multivariate Analysis](#Multivariate)\n",
    "    - [Takeaway](#Takeaway3)\n",
    "    - [Outliers](#Outliers)\n",
    "    \n",
    "7. [Feature Engineering](#Feature)\n",
    "    - [Add new Features](#Feature)\n",
    "    - [Remove Features](#Feature1)\n",
    "    - [Apply Transformations](#Feature12)\n",
    "    - [Convert catagorical variables](#Feature2)\n",
    "    - [Standardize](#Feature3)\n",
    "    - [Normalize](#Feature4)\n",
    "    - [Make Binary](#Feature5)\n",
    "    - [Scale](#Feature6)\n",
    "    - [Bin](#Feature7)\n",
    "    - [Correlation and Interaction variables](#Feature8)\n",
    "    - [Testing](#Feature9)\n",
    "    - [Repeat EDA](#Feature10)\n",
    "    - [Takeaway](#Feature11)\n",
    "  \n",
    "8. [Feature Selection](#Feature-Selection)\n",
    "\n",
    "9. [Algorithm Evaluation With Resampling Methods](#Resampling)\n",
    "\n",
    "10. [Performance metrics](#metrics)\n",
    "\n",
    "11. [Spot-check Algorithms](#Spot-check)\n",
    "\n",
    "12. [Hyper-parameter Optimisation](#Hyperparameters)\n",
    "\n",
    "13. [A single Pipeline](#Pipeline)\n",
    "\n",
    "14. [Finalize the model](#Pickle)\n",
    "\n",
    "15. [If I had more time..](#time)\n",
    "\n",
    "16. [Acknowledgements](#Acknowledgement)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Problem'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Definition\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "#### STEP 1 : What is the problem?\n",
    "Informal description\n",
    "For example: I need a program that will tell me which tweets will get retweets.\n",
    "###### Formalism\n",
    "A Computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.\n",
    "For example:\n",
    "Task (T): Classify a tweet that has not been published as going to get retweets or not.\n",
    "Experience (E): A corpus of tweets for an account where some have retweets and some do not.\n",
    "Performance (P): Classification accuracy, the number of tweets predicted correctly out of all tweets considered as a percentage.\n",
    "###### Assumptions\n",
    "For example:\n",
    "The specific words used in the tweet matter to the model.\n",
    "The specific user that retweets does not matter to the model.\n",
    "The number of retweets may matter to the model.\n",
    "Older tweets are less predictive than more recent tweets.\n",
    "###### Similar problems\n",
    "For example: A related problem would be email spam discrimination that uses text messages as input data and needs binary classification decision.\n",
    "#### STEP 2 : Why does the problem need to be solved?\n",
    "Motivation\n",
    "Solution Benefits\n",
    "Solution Use\n",
    "#### STEP 3 : How would I solve the problem?\n",
    "List out step-by-step what data you would collect, how you would prepare it and how you would design a program to solve the problem. This may include prototypes and experiments you would need to perform which are a gold mine because they will highlight questions and uncertainties you have about the domain that could be explored.\n",
    "This is a powerful tool. It can highlight problems that actually can be solved satisfactorily using a manually implemented solution. It also flushes out important domain knowledge that has been trapped up until now like where the data is actually stored, what types of features would be useful and many other details.\n",
    "Collect all of these details as they occur to you and update the previous sections of the problem definition. Especially the assumptions and rules of thumb.\n",
    "#### Data Analysis Checklist \n",
    "The first step to any data analysis project is to define the question or problem we're looking to solve, and to define a measure (or set of measures) for our success at solving that task. The data analysis checklist has us answer a handful of questions to accomplish that, so let's work through those questions.<br>\n",
    "<b>Did you specify the type of data analytic question (e.g. exploration, association causality) before touching the data?</b><br>\n",
    "We're trying to classify the species (i.e., class) of the flower based on four measurements that we're provided: sepal length, sepal width, petal length, and petal width.<br>\n",
    "<b>Did you define the metric for success before beginning?</b><br>\n",
    "Let's do that now. Since we're performing classification, we can use accuracy — the fraction of correctly classified flowers — to quantify how well our model is performing. Our head of data has told us that we should achieve at least 90% accuracy.<br>\n",
    "<b>Did you understand the context for the question and the scientific or business application?</b><br>\n",
    "We're building part of a data analysis pipeline for a smartphone app that will be able to classify the species of flowers from pictures taken on the smartphone. In the future, this pipeline will be connected to another pipeline that automatically measures from pictures the traits we're using to perform this classification.<br>\n",
    "<b>Did you record the experimental design?</b><br>\n",
    "Our head of data has told us that the field researchers are hand-measuring 50 randomly-sampled flowers of each species using a standardized methodology. The field researchers take pictures of each flower they sample from pre-defined angles so the measurements and species can be confirmed by the other field researchers at a later point. At the end of each day, the data is compiled and stored on a private company GitHub repository.<br>\n",
    "<b>Did you consider whether the question could be answered with the available data?</b><br>\n",
    "The data set we currently have is only for three types of Iris flowers. The model built off of this data set will only work for those Iris flowers, so we will need more data to create a general flower classifier.<br>\n",
    "\n",
    "Notice that we've spent a fair amount of time working on the problem without writing a line of code or even looking at the data.\n",
    "Thinking about and documenting the problem we're working on is an important step to performing effective data analysis that often goes overlooked. Don't skip it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Required-libraries'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required Libraries\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python version\n",
    "import sys\n",
    "print('Python: {}'.format(sys.version))\n",
    "# scipy\n",
    "import scipy\n",
    "print('scipy: {}'.format(scipy.__version__))\n",
    "# numpy\n",
    "import numpy\n",
    "print('numpy: {}'.format(numpy.__version__))\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "print('matplotlib: {}'.format(matplotlib.__version__))\n",
    "# pandas\n",
    "import pandas\n",
    "print('pandas: {}'.format(pandas.__version__))\n",
    "# scikit-learn\n",
    "import sklearn\n",
    "print('sklearn: {}'.format(sklearn.__version__))\n",
    "import seaborn as sns\n",
    "print('seaborn: {}'.format(seaborn.__version__))\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Load-data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "Example code :<br> \n",
    "\n",
    "energy = pd.read_excel(\"Energy Indicators.xls\",sheetname=0,skiprows=17,skip_footer=38 )<br>\n",
    "energy = (energy.drop(energy.columns[[0, 1]], axis=1).rename(columns={'Unnamed: 2': 'Country'}).replace('...',np.nan) .set_index(['Country']))<br>\n",
    "GDP = pd.read_csv('world_bank.csv',skiprows=4,encoding='utf-8-sig')<br>\n",
    "GDP = GDP.rename(columns={'Country Name':'Country'}).set_index(['Country'])<br>\n",
    "GDP = GDP[GDP.columns[-10:]].copy()<br>  \n",
    "GDP.index = GDP.index.str.replace('Korea, Rep.','South Korea')<br>\n",
    "ScimEn = pd.read_excel('scimagojr-3.xlsx',sheetname=0,index_col=1,encoding='utf-8-sig')<br>\n",
    "join_df_full = pd.merge(ScimEn,GDP, how='left', left_index=True, right_index=True)<br>\n",
    "join_df_full = pd.merge(join_df_full,energy,how='left', left_index=True, right_index=True)<br>\n",
    "full_rows,full_columns = join_df_full.shape <br>\n",
    "<br>\n",
    "<br>\n",
    "One of the first things we should look for is missing data. If the field researchers told us that they put a 'NA' into the spreadsheet when they were missing a measurement.We can tell pandas to automatically identify missing values if it knows our missing value marker.<br>\n",
    "example : iris_data = pd.read_csv('iris-data.csv', na_values=['NA'])<br>\n",
    "<br>\n",
    "We can also do some data-cleanup at this point<br>\n",
    "energy.index = energy.index.str.replace(r'\\d+','')<br>\n",
    "energy.index = energy.index.str.replace(r'\\s\\(.*\\)','')<br>\n",
    "energy.index = energy.index.str.replace('Republic of Korea','South Korea')<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Checking-the-data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the data\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "The next step is to look at the data we're working with. Even curated data sets from the government can have errors in them, and it's vital that we spot these errors before investing too much time in our analysis.<br>\n",
    "Generally, we're looking to answer the following questions:<br>\n",
    "Is there anything wrong with the data?<br>\n",
    "Are there any quirks with the data?<br>\n",
    "Do I need to fix or remove any of the data?<br>\n",
    "<br>\n",
    "For small datasets visualize using a pairplot to check for weirdness in the data , get the overall picture check if the data is linearly separable.<br>\n",
    "example : sb.pairplot(iris_data.dropna(), hue='class')<br>\n",
    "For larger datasets with lots of dimensions use Dimentionality Reduction techniques to visualize after variable conversion<br>\n",
    "<b>PCA</b><br>\n",
    "<b>t-SNE</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Peek'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Peek at your data \n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "Use head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='#Dimensions'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Dimensions\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "Use df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='#Datatype'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Datatype of each feature\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "Use info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='#Descriptive-stats'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Descriptive Statistics\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "Use describe() & describe(include=[‘O’])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='#Class-dist'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 : Class Distribution\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "class_counts = df.groupby('class').size() <br>\n",
    "Classification problems with imbalance classes require special treatment<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='#Correlation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 : Correlation\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "correlations = df.corr(method='pearson')<br>\n",
    "If you have ordinal data, you will want to use Spearman's rank-order correlation or a Kendall's Tau Correlation instead of the Pearson product-moment correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='#Skew'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 : Skew\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "skew = df.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='#Takeaway1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaway from \"Checking the data\"\n",
    "What did I learn? <br>\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "##### What is the distribution of numerical feature values across the samples?\n",
    "##### What is the distribution of categorical feature values across the samples?\n",
    "##### Which features are missing data?\n",
    "##### Can anomalies/outliers/anything weird be spotted?\n",
    "### Assumtions based on data analysis\n",
    "We arrive at following assumptions based on data analysis done so far. We may validate these assumptions further before taking appropriate actions.\n",
    "###### Correlating.\n",
    "We want to know how well does each feature correlate with Survival. We want to do this early in our project and match these quick correlations with modelled correlations later in the project.\n",
    "###### Completing.\n",
    "We may want to complete Age feature as it is definitely correlated to survival.<br>\n",
    "We may want to complete the Embarked feature as it may also correlate with survival or another important feature.\n",
    "###### Correcting.\n",
    "Ticket feature may be dropped from our analysis as it contains high ratio of duplicates (22%) and there may not be a correlation between Ticket and survival.<br>\n",
    "Cabin feature may be dropped as it is highly incomplete or contains many null values both in training and test dataset.<br>\n",
    "PassengerId may be dropped from training dataset as it does not contribute to survival.<br>\n",
    "Name feature is relatively non-standard, may not contribute directly to survival, so maybe dropped.<br>\n",
    "###### Creating.\n",
    "We may want to create a new feature called Family based on Parch and SibSp to get total count of family members on board.<br>\n",
    "We may want to engineer the Name feature to extract Title as a new feature.<br>\n",
    "We may want to create new feature for Age bands. This turns a continous numerical feature into an ordinal categorical feature.<br>\n",
    "We may also want to create a Fare range feature if it helps our analysis.<br>\n",
    "###### Classifying.\n",
    "We may also add to our assumptions based on the problem description noted earlier.<br>\n",
    "Women (Sex=female) were more likely to have survived.<br>\n",
    "Children (Age<?) were more likely to have survived.<br> \n",
    "The upper-class passengers (Pclass=1) were more likely to have survived.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Missing-data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing data\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)<br>\n",
    "Some predictive models inherently are able to deal with missing data (neural networks come to mind) and others require that the missing values be dealt with separately.<br>\n",
    "### Identifying Missing data\n",
    "We can use seaborn to create a simple heatmap to see where we are missing data!<br>\n",
    "sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking Care with Missing values\n",
    "1) Throw out any data with missing values<br>\n",
    "If you’ve got a lot of data that isn’t missing any values it is certainly the quickest and easiest way to handle it.\n",
    "<br>\n",
    "train.drop('Cabin',axis=1,inplace=True)<br>\n",
    "2) Assign a value that indicates a missing value<br>\n",
    "This is particularly appropriate for categorical variables (more on this in the next post). I really like using this approach when possible because the fact that the value is missing can be useful information in and of itself. Perhaps when a value is missing for a particular variable, that has some underlying cause that makes it correlate more highly with another value.One interesting trick is that you can do this with binary variables by setting the false value as -1, the true value as 1, and missing values as 0.<br>\n",
    "df['Cabin'][df.Cabin.isnull()] = 'U0'<br>\n",
    "3) Assign the average value<br>\n",
    "This is a very common approach because it is simple, and for variables that aren’t extremely important it very well may be good enough. You can also incorporate other variables to create subsets and assign the average within the group. In cases of categorical variables, the most common value can be applied rather than the statistical mean.<br>\n",
    "df['Fare'][ np.isnan(df['Fare']) ] = df['Fare'].median()<br>\n",
    "df.Embarked[ df.Embarked.isnull() ] = df.Embarked.dropna().mode().values <br>\n",
    "4) Use a regression or another simple model to predict the values of missing variables <br>\n",
    "Useful code :<br>\n",
    "from sklearn.ensemble import RandomForestRegressor (Can be LinearRegression as well) <br>\n",
    " \n",
    "def setMissingAges(df):\n",
    "    \n",
    "    # Grab all the features that can be included in a Random Forest Regressor\n",
    "    age_df = df[['Age','Embarked','Fare', 'Parch', 'SibSp', 'Title_id','Pclass','Names','CabinLetter']]\n",
    "    \n",
    "    # Split into sets with known and unknown Age values\n",
    "    knownAge = age_df.loc[ (df.Age.notnull()) ]\n",
    "    unknownAge = age_df.loc[ (df.Age.isnull()) ]\n",
    "    \n",
    "    # All age values are stored in a target array\n",
    "    y = knownAge.values[:, 0]\n",
    "    \n",
    "    # All the other values are stored in the feature array\n",
    "    X = knownAge.values[:, 1::]\n",
    "    \n",
    "    # Create and fit a model\n",
    "    rtr = RandomForestRegressor(n_estimators=2000, n_jobs=-1)\n",
    "    rtr.fit(X, y)\n",
    "    \n",
    "    # Use the fitted model to predict the missing values\n",
    "    predictedAges = rtr.predict(unknownAge.values[:, 1::])\n",
    "    \n",
    "    # Assign those predictions to the full data set\n",
    "    df.loc[ (df.Age.isnull()), 'Age' ] = predictedAges \n",
    "    \n",
    "    return df\n",
    "<br>\n",
    "def impute_age(cols):\n",
    "    Age = cols[0]\n",
    "    Pclass = cols[1]\n",
    "    \n",
    "    if pd.isnull(Age):\n",
    "\n",
    "        if Pclass == 1:\n",
    "            return 37\n",
    "\n",
    "        elif Pclass == 2:\n",
    "            return 29\n",
    "\n",
    "        else:\n",
    "            return 24\n",
    "\n",
    "    else:\n",
    "        return Age\n",
    "train['Age'] = train[['Age','Pclass']].apply(impute_age,axis=1)\n",
    "\n",
    "5)Impute missing data using a reliable method, such as k-nearest neighbors.<br>\n",
    "<br>\n",
    "from sklearn.preprocessing import Imputer <br>\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)<br>\n",
    "X = imp.fit_transform(X)<br>\n",
    "<br>\n",
    "The Imputer class provides basic strategies for imputing missing values, either using the mean, the median or the most frequent value of the row or column in which the missing values are located. This class also allows for different missing values encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recheck if Missing data is dealt with\n",
    "We can use seaborn to create the heatmap again to see whether we are still missing data.<br>\n",
    "sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)<br>\n",
    "This is best done by Visualizations<br>\n",
    "### Univariate Analysis\n",
    "##### What is the plan ? Based on what happened so far?\n",
    "<br>\n",
    "#### Visualizations\n",
    "##### Histograms <br>\n",
    "A fast way to get an idea of the distribution of each attribute is to look at histograms. Histograms group data into bins and provide you a count of the number of observations in each bin. From the shape of the bins you can quickly get a feeling for whether an attribute is Gaussian’, skewed or even has an exponential distribution. It can also help you see possible outliers.<br>\n",
    "Bins should be all the same size. For example, groups of ten or a hundred.<br>\n",
    "Bins should include all of the data, even outliers. If your outliers fall way outside of your other data, consider lumping them in with your first or last bin. This creates a rough histogram — make sure you note where outliers are being included.<br>\n",
    "Boundaries for bins should land at whole numbers whenever possible (this makes the chart easier to read).<br>\n",
    "Choose between 5 and 20 bins. The larger the data set, the more likely you’ll want a large number of bins. For example, a set of 12 data pieces might warrant 5 bins but a set of 1000 numbers will probably be more useful with 20 bins. The exact number of bins is usually a judgment call.<br>\n",
    "If at all possible, try to make your data set evenly divisible by the number of bins. For example, if you have 10 pieces of data, work with 5 bins instead of 6 or 7.<br>\n",
    "One goal would be to minimize the integrated mean square error.  When the distribution is normal, this can be done with the Freedman–Diaconis rule of course, one might listent to William S. Cleveland (perhaps the world's leading expert on statistical graphics) and not use histograms at all.\n",
    "Useful code : <br>\n",
    "sns.set_style('whitegrid')<br>\n",
    "sns.countplot(x='Survived',data=train,palette='RdBu_r')<br>\n",
    "train['Age'].hist(bins=30,color='darkred',alpha=0.7)<br>\n",
    "sns.distplot(train['Age'].dropna(),kde=False,color='darkred',bins=30)<br>\n",
    "train['Fare'].hist(color='green',bins=40,figsize=(8,4))\n",
    "##### Density Plots\n",
    "Density plots are another way of getting a quick idea of the distribution of each attribute. The plots look like an abstracted histogram with a smooth curve drawn through the top of each bin, much like your eye tried to do with the histograms.<br>\n",
    "sns.distplot(train['Age'].dropna(),kde=True,color='darkred',bins=30)<br>\n",
    "##### Box and Whisker Plots\n",
    "Another useful way to review the distribution of each attribute is to use Box and Whisker Plots or boxplots for short.\n",
    "Boxplots summarize the distribution of each attribute, drawing a line for the median (middle value) and a box around the 25th and 75th percentiles (the middle 50% of the data). The whiskers give an idea of the spread of the data and dots outside of the whiskers show candidate outlier values (values that are 1.5 times greater than the size of spread of the middle 50% of the data).<br>\n",
    "plt.figure(figsize=(12, 7))<br>\n",
    "sns.boxplot(x='Pclass',y='Age',data=train,palette='winter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='#Takeaway2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaway from \"Univariate Analysis\"\n",
    "What did I learn? <br>\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "##### What is the distribution of numerical feature values across the samples?\n",
    "This helps us determine, among other early insights, how representative is the training dataset of the actual problem domain.<br>\n",
    "Total samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224).<br>\n",
    "Survived is a categorical feature with 0 or 1 values.<br>\n",
    "Around 38% samples survived representative of the actual survival rate at 32%.<br>\n",
    "Most passengers (> 75%) did not travel with parents or children.<br>\n",
    "Nearly 30% of the passengers had siblings and/or spouse aboard.<br>\n",
    "Fares varied significantly with few passengers ( < 1 percent) paying as high as 512.<br>\n",
    "Few elderly passengers ( < 1 percent) within age range 65-80. <br>\n",
    "##### What is the distribution of categorical feature values across the samples?\n",
    "Names are unique across the dataset (count=unique=891)<br>\n",
    "Sex variable as two possible values with 65% male (top=male, freq=577/count=891).<br>\n",
    "Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin.<br>\n",
    "Embarked takes three possible values. S port used by most passengers (top=S)<br>\n",
    "Ticket feature has high ratio (22%) of duplicate values (unique=681).<br>\n",
    "##### Can anomalies/outliers/anything weird be spotted?\n",
    "\n",
    "##### If yes(for the previous question) ,what is the plan to handle anomalies/outliers/anything weird that has be spotted?\n",
    "\n",
    "### Assumtions based on data analysis\n",
    "We arrive at following assumptions based on data analysis done so far. We may validate these assumptions further before taking appropriate actions.\n",
    "###### Correlating.\n",
    "We want to know how well does each feature correlate with Survival. We want to do this early in our project and match these quick correlations with modelled correlations later in the project.\n",
    "###### Completing.\n",
    "We may want to complete Age feature as it is definitely correlated to survival.<br>\n",
    "We may want to complete the Embarked feature as it may also correlate with survival or another important feature.\n",
    "###### Correcting.\n",
    "Ticket feature may be dropped from our analysis as it contains high ratio of duplicates (22%) and there may not be a correlation between Ticket and survival.<br>\n",
    "Cabin feature may be dropped as it is highly incomplete or contains many null values both in training and test dataset.<br>\n",
    "PassengerId may be dropped from training dataset as it does not contribute to survival.<br>\n",
    "Name feature is relatively non-standard, may not contribute directly to survival, so maybe dropped.<br>\n",
    "###### Creating.\n",
    "We may want to create a new feature called Family based on Parch and SibSp to get total count of family members on board.<br>\n",
    "We may want to engineer the Name feature to extract Title as a new feature.<br>\n",
    "We may want to create new feature for Age bands. This turns a continous numerical feature into an ordinal categorical feature.<br>\n",
    "We may also want to create a Fare range feature if it helps our analysis.<br>\n",
    "###### Classifying.\n",
    "We may also add to our assumptions based on the problem description noted earlier.<br>\n",
    "Women (Sex=female) were more likely to have survived.<br>\n",
    "Children (Age<?) were more likely to have survived.<br> \n",
    "The upper-class passengers (Pclass=1) were more likely to have survived.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='#Multivariate'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Analysis\n",
    "[[ go back to the top ]](#Table-of-contents)<br>\n",
    "##### What is the plan ? Based on what happened so far?\n",
    "<br>\n",
    "This is best done by Visualizations and by feature correlations by pivoting features against each other<br>\n",
    "##### Analyze by pivoting features\n",
    "To confirm some of our observations and assumptions, we can quickly analyze our feature correlations by pivoting features against each other.<br>\n",
    "It also makes sense doing so only for features which are categorical (Sex), ordinal (Pclass) or discrete (SibSp, Parch) type.<br>\n",
    "Pclass We observe significant correlation (>0.5) among Pclass=1 and Survived (classifying # 3). We decide to include this feature in our model.<br>\n",
    "Sex We confirm the observation during problem definition that Sex=female had very high survival rate at 74% (classifying # 1).<br>\n",
    "SibSp and Parch These features have zero correlation for certain values. It may be best to derive a feature or a set of features from these individual features (creating # 1).<br>\n",
    "Sample code :<br>\n",
    "train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n",
    "##### Visualizations\n",
    "For<b> Large Datasets use Dimentionality Reduction Techniques </b> like PCA and t-SNE to viaualize data\n",
    "<br>\n",
    "###### PCA\n",
    "from sklearn.preprocessing import StandardScaler<br>\n",
    "scaler = StandardScaler()<br>\n",
    "scaler.fit(df)<br>\n",
    "scaled_data = scaler.transform(df)<br>\n",
    "from sklearn.decomposition import PCA<br>\n",
    "pca = PCA(n_components=2)<br>\n",
    "pca.fit(scaled_data)<br>\n",
    "x_pca = pca.transform(scaled_data)<br>\n",
    "scaled_data.shape<br>\n",
    "x_pca.shape<br>\n",
    "plt.figure(figsize=(8,6))<br>\n",
    "plt.scatter(x_pca[:,0],x_pca[:,1],c=cancer['target'],cmap='plasma')<br>\n",
    "plt.xlabel('First principal component')<br>\n",
    "plt.ylabel('Second Principal Component')<br>\n",
    "#The components correspond to combinations of the original features, the components themselves are stored as an attribute of the fitted PCA object:<br>\n",
    "pca.components_ <br>\n",
    "df_comp = pd.DataFrame(pca.components_,columns=cancer['feature_names'])<br>\n",
    "plt.figure(figsize=(12,6))<br>\n",
    "sns.heatmap(df_comp,cmap='plasma',)<br>\n",
    "This heatmap and the color bar basically represent the correlation between the various feature and the principal component itself.<br>\n",
    "###### t-SNE \n",
    "When PCA doesnt work ..when we need a non-linear dimentionality reduction.It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples.<br>\n",
    "from sklearn.manifold import TSNE<br>\n",
    "X_tsne = TSNE(learning_rate=100).fit_transform(iris.data)<br>\n",
    "X_pca = PCA().fit_transform(iris.data)<br>\n",
    "figure(figsize=(10, 5))<br>\n",
    "subplot(121)<br>\n",
    "scatter(X_tsne[:, 0], X_tsne[:, 1], c=iris.target)<br>\n",
    "subplot(122)<br>\n",
    "scatter(X_pca[:, 0], X_pca[:, 1], c=iris.target)<br>\n",
    "(Vary the colour of the points with different features...Color-coherent clusters shows that for T-SNE, nothing is closer to a given observation than an observation with that the similar feature . This is also a consequence of the T-SNE algorithm itself, that focuses on preserving very small pairwise distances between points, contrary to PCA.To spot abnormal points : those with color that do not correspond to their cluster. They are anomalies.<br>\n",
    "###### Pairplots (for small datasets)\n",
    "sns.pairplot(df,hue='Kyphosis',palette='Set1')<br>\n",
    "sns.pairplot(iris,hue='species',palette='Dark2')<br>\n",
    "sb.pairplot(iris_data.dropna(), hue='class')\n",
    "<br>\n",
    "scatter_matrix(dataset)<br>\n",
    "plt.show()<br>\n",
    "###### Countplots with hue\n",
    "sns.set_style('whitegrid')<br>\n",
    "sns.countplot(x='Survived',hue='Sex',data=train,palette='RdBu_r')<br>\n",
    "ns.set_style('whitegrid')\n",
    "sns.countplot(x='Survived',hue='Pclass',data=train,palette='rainbow')<br>\n",
    "###### Jointplot (scatterplot)\n",
    "sns.jointplot(x='fico',y='int.rate',data=loans,color='purple')<br>\n",
    "sns.jointplot(x='Time on Website',y='Yearly Amount Spent',data=customers)<br>\n",
    "sns.jointplot(x='Time on App',y='Length of Membership',kind='hex',data=customers)<br>\n",
    "<br>\n",
    "plt.scatter(data[0][:,0],data[0][:,1],c=data[1],cmap='rainbow')<br>\n",
    "<br>\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True,figsize=(10,6))<br>\n",
    "ax1.set_title('K Means')<br>\n",
    "ax1.scatter(data[0][:,0],data[0][:,1],c=kmeans.labels_,cmap='rainbow')<br>\n",
    "ax2.set_title(\"Original\")<br>\n",
    "ax2.scatter(data[0][:,0],data[0][:,1],c=data[1],cmap='rainbow')<br>\n",
    "###### lmplots\n",
    "plt.figure(figsize=(11,7))<br>\n",
    "sns.lmplot(y='int.rate',x='fico',data=loans,hue='credit.policy',col='not.fully.paid',palette='Set1')<br>\n",
    "<br>\n",
    "sns.set_style('whitegrid')<br>\n",
    "sns.lmplot('Room.Board','Grad.Rate',data=df, hue='Private',<br>\n",
    "           palette='coolwarm',size=6,aspect=1,fit_reg=False)<br>\n",
    "<br>\n",
    "###### Overlapping histograms\n",
    "ns.set_style('darkgrid')<br>\n",
    "g = sns.FacetGrid(df,hue=\"Private\",palette='coolwarm',size=6,aspect=2)<br>\n",
    "g = g.map(plt.hist,'Outstate',bins=20,alpha=0.7)<br>\n",
    "<br>\n",
    "###### Heatmaps\n",
    "sns.heatmap(USAhousing.corr())<br>\n",
    "<br>\n",
    "plt.figure(figsize=(12,6))<br>\n",
    "sns.heatmap(df_comp,cmap='plasma',)\n",
    "<br>\n",
    "###### kdeplot\n",
    "sns.kdeplot( setosa['sepal_width'], setosa['sepal_length'],<br>\n",
    "                 cmap=\"plasma\", shade=True, shade_lowest=False)\n",
    "<br>\n",
    "###### violinplot\n",
    "plt.figure(figsize=(10, 10))<br>\n",
    "\n",
    "for column_index, column in enumerate(iris_data_clean.columns):<br>\n",
    "    if column == 'class': <br>\n",
    "        continue <br>\n",
    "    plt.subplot(2, 2, column_index + 1) <br>\n",
    "    sb.violinplot(x='class', y=column, data=iris_data_clean) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='#Takeaway3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaway from \"Multivariate Analysis\"\n",
    "What did I learn? <br>\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "##### What is the distribution of numerical feature values across the samples?\n",
    "This helps us determine, among other early insights, how representative is the training dataset of the actual problem domain.<br>\n",
    "Total samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224).<br>\n",
    "Survived is a categorical feature with 0 or 1 values.<br>\n",
    "Around 38% samples survived representative of the actual survival rate at 32%.<br>\n",
    "Most passengers (> 75%) did not travel with parents or children.<br>\n",
    "Nearly 30% of the passengers had siblings and/or spouse aboard.<br>\n",
    "Fares varied significantly with few passengers ( < 1 percent) paying as high as 512.<br>\n",
    "Few elderly passengers ( < 1 percent) within age range 65-80. <br>\n",
    "##### What is the distribution of categorical feature values across the samples?\n",
    "Names are unique across the dataset (count=unique=891)<br>\n",
    "Sex variable as two possible values with 65% male (top=male, freq=577/count=891).<br>\n",
    "Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin.<br>\n",
    "Embarked takes three possible values. S port used by most passengers (top=S)<br>\n",
    "Ticket feature has high ratio (22%) of duplicate values (unique=681).<br>\n",
    "##### Can anomalies/outliers/anything weird be spotted?\n",
    "\n",
    "##### If yes(for the previous question) ,what is the plan to handle anomalies/outliers/anything weird that has be spotted?\n",
    "### Assumtions based on data analysis\n",
    "We arrive at following assumptions based on data analysis done so far. We may validate these assumptions further before taking appropriate actions.\n",
    "###### Correlating.\n",
    "We want to know how well does each feature correlate with Survival. We want to do this early in our project and match these quick correlations with modelled correlations later in the project.\n",
    "###### Completing.\n",
    "We may want to complete Age feature as it is definitely correlated to survival.<br>\n",
    "We may want to complete the Embarked feature as it may also correlate with survival or another important feature.\n",
    "###### Correcting.\n",
    "Ticket feature may be dropped from our analysis as it contains high ratio of duplicates (22%) and there may not be a correlation between Ticket and survival.<br>\n",
    "Cabin feature may be dropped as it is highly incomplete or contains many null values both in training and test dataset.<br>\n",
    "PassengerId may be dropped from training dataset as it does not contribute to survival.<br>\n",
    "Name feature is relatively non-standard, may not contribute directly to survival, so maybe dropped.<br>\n",
    "###### Creating.\n",
    "We may want to create a new feature called Family based on Parch and SibSp to get total count of family members on board.<br>\n",
    "We may want to engineer the Name feature to extract Title as a new feature.<br>\n",
    "We may want to create new feature for Age bands. This turns a continous numerical feature into an ordinal categorical feature.<br>\n",
    "We may also want to create a Fare range feature if it helps our analysis.<br>\n",
    "###### Classifying.\n",
    "We may also add to our assumptions based on the problem description noted earlier.<br>\n",
    "Women (Sex=female) were more likely to have survived.<br>\n",
    "Children (Age<?) were more likely to have survived.<br> \n",
    "The upper-class passengers (Pclass=1) were more likely to have survived.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='#Outliers'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "[[ go back to the top ]](#Table-of-contents)<br>\n",
    "#### Here are some changes you can make to your model:\n",
    "\n",
    "<b>Use a model that's resistant to outliers.</b><br>\n",
    "Tree-based models are generally not as affected by outliers, while regression-based models are. If you're performing a statistical test, try a non-parametric test instead of a parametric one.\n",
    "<br>\n",
    "<b>Use a more robust error metric.</b><br>\n",
    "Switching from mean squared error to mean absolute difference (or something like Huber Loss) reduces the influence of outliers. Why is the median a measure of central tendency?\n",
    "<br>\n",
    "<b>Here are some changes you can make to your data:</b><br>\n",
    "<br>\n",
    "Winsorize your data. Artificially cap your data at some threshold. <br>\n",
    "Transform your data. If your data has a very pronounced right tail, try a log transformation.<br>\n",
    "Remove the outliers. This works if there are very few of them and you're fairly certain they're anomalies and not worth predicting.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Feature'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering : Add new features\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "##### Derived Features\n",
    "Very basic examples of a useful derived variable might be pulling the country code and/or area code out of telephone numbers, or extracting country/state/city from GPS coordinates. Any time a qualitative variable represents an object in the world that we know something about, there is an opportunity to derive variables from it. Also, if a data set represents a timeseries or other historical behavioral information that can also provide a great opportunity for uncovering derived variables.<br>\n",
    "##### Decomposition\n",
    "There may be features that represent a complex concept that may be more useful to a machine learning method when split into the constituent parts. An example is a date that may have day and time components that in turn could be split out further. Perhaps only the hour of day is relevant to the problem being solved. consider what feature decompositions you can perform.\n",
    "##### Aggregation\n",
    "There may be features that can be aggregated into a single feature that would be more meaningful to the problem you are trying to solve. For example, there may be a data instances for each time a customer logged into a system that could be aggregated into a count for the number of logins allowing the additional instances to be discarded. Consider what type of feature aggregations could perform.\n",
    "<br> Examples - \n",
    "###### 1. Create variables for difference in date, time and addresses<br>\n",
    "While you might be using date and time values on their own, you can create new variables by considering differences in dates and time. Here is an example hypothesis: An applicant who takes days to fill in an application form is likely to be less interested / motivated in the product compared to some one who fills in the same application with in 30 minutes. Similarly, for a bank, time elapsed between dispatch of login details for Online portal and customer logging in might show customers’ willingness to use Online portal.<br>\n",
    "<br>\n",
    "Another example is that a customer living closer to a bank branch is more likely to have a higher engagement than a customer living far off.<br>\n",
    "<br>\n",
    "###### 2. Create new ratios and proportions<br>\n",
    "Instead of just keeping past inputs and outputs in your dataset, creating ratios out of them might add a lot of value. Some of the ratios, I have used in past are: Input / Output (past performance), productivity, efficiency and percentages. For example, in order to predict future performance of credit card sales of a branch, ratios like credit card sales / Sales person or Credit card Sales / Marketing spend would be more powerful than just using absolute number of card sold in the branch<br>\n",
    "###### 3. Include effect of influencer<br>\n",
    "Influencer can impact on the behaviour of your study significantly. Influencer could be of various form and sizes. It could be an employee of your Organization, agent of your Organization or a customer of your Organization. Bringing the impact of these related entities can improve the models significantly. For example, a loan initiated by a sub-set of brokers (and not all brokers) might be more likely to be transferred to a different entity after the lock-in period. Similarly, there might be a sub set of Sales personnel involved who do a higher cross-sell to their customers.<br>\n",
    "<br>\n",
    "###### 4. Check variables for seasonality and create the model for right period<br>\n",
    "A lot of businesses face some kind of seasonality. It could be driven by tax benefits, festive season or weather. If this is the case, you need to make sure that the data and variables are chosen for the right period.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Feature1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering : Remove Features\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "<b>Useless Attributes</b>\n",
    "<b>Correlated Attributes:</b> Some algorithms degrade in importance with the existence of highly correlated attributes. Pairwise attributes with high correlation can be identified and the most correlated attributes can be removed from the data.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Feature12'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering : Apply Transformations\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "When we can transform complex non-linear relationships into linear relationships. Existence of a linear relationship between variables is easier to comprehend compared to a non-linear or curved relation. Transformation helps us to convert a non-linear relation into linear relation. Scatter plot can be used to find the relationship between two continuous variables. These transformations also improve the prediction. Log transformation is one of the commonly used transformation technique used in these situations.<br>\n",
    "Symmetric distribution is preferred over skewed distribution as it is easier to interpret and generate inferences. Some modeling techniques requires normal distribution of variables. So, whenever we have a skewed distribution, we can use transformations which reduce skewness. For right skewed distribution, we take square / cube root or logarithm of variable and for left skewed, we take square / cube or exponential of variables.<br>\n",
    "Variable Transformation is also done from an implementation point of view (Human involvement). Let’s understand it more clearly. In one of my project on employee performance, I found that age has direct correlation with performance of the employee i.e. higher the age, better the performance. From an implementation stand point, launching age based progamme might present implementation challenge. However, categorizing the sales agents in three age group buckets of < 30 years, 30-45 years and >45  and then formulating three different strategies for each group is a judicious approach. This categorization technique is known as Binning of Variables.<br>\n",
    "There are various methods used to transform variables. As discussed, some of them include square root, cube root, logarithmic, binning, reciprocal and many others. Let’s look at these methods in detail by highlighting the pros and cons of these transformation methods.<br>\n",
    "<b>Logarithm: </b>Log of a variable is a common transformation method used to change the shape of distribution of the variable on a distribution plot. It is generally used for reducing right skewness of variables. Though, It can’t be applied to zero or negative values as well.Log(Marketing spend) might have a more representable relationship with Sales as compared to absolute Marketing spend.<br>\n",
    "<b>Square / Cube root: </b>The square and cube root of a variable has a sound effect on variable distribution. However, it is not as significant as logarithmic transformation. Cube root has its own advantage. It can be applied to negative <b>Binning: </b>It is used to categorize variables. It is performed on original values, percentile or frequency. Decision of categorization technique is based on business understanding. For example, we can categorize income in three categories, namely: High, Average and Low. We can also perform co-variate binning which depends on the value of more than one variables.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Feature2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering : Convert Categorical Variables \n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "Logistic regression, distance based methods such as kNN, support vector machines, tree based methods etc. in sklearn needs numeric arrays. Features having string values cannot be handled by these learners.<br>\n",
    "Sklearn provides a very efficient tools for encoding the levels of a categorical features into numeric values.<br>\n",
    "##### LabelEncoder\n",
    "LabelEncoder encode labels with value between 0 and n_classes-1.<br>\n",
    "Lets encode all the categorical features.<br>\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder<br>\n",
    "le=LabelEncoder()<br>\n",
    "for col in X_test.columns.values:<br>\n",
    "       # Encoding only categorical variables\n",
    "       if X_test[col].dtypes=='object':\n",
    "       #Using whole data to form an exhaustive list of levels\n",
    "       data=X_train[col].append(X_test[col])\n",
    "       le.fit(data.values)\n",
    "       X_train[col]=le.transform(X_train[col])\n",
    "       X_test[col]=le.transform(X_test[col])\n",
    "<br>\n",
    "###### One-hot-encoding\n",
    "       from sklearn.preprocessing import OneHotEncoder<br>\n",
    "       enc=OneHotEncoder(sparse=False)\n",
    "       X_train_1=X_train\n",
    "       X_test_1=X_test\n",
    "       columns=['Gender', 'Married', 'Dependents', 'Education','Self_Employed',\n",
    "          'Credit_History', 'Property_Area']\n",
    "       for col in columns:\n",
    "       # creating an exhaustive list of all possible categorical values\n",
    "       data=X_train[[col]].append(X_test[[col]])\n",
    "       enc.fit(data)\n",
    "       # Fitting One Hot Encoding on train data\n",
    "       temp = enc.transform(X_train[[col]])\n",
    "       # Changing the encoded features into a data frame with new column names\n",
    "       temp=pd.DataFrame(temp,columns=[(col+\"_\"+str(i)) for i in data[col]\n",
    "            .value_counts().index])\n",
    "       # In side by side concatenation index values should be same\n",
    "       # Setting the index values similar to the X_train data frame\n",
    "       temp=temp.set_index(X_train.index.values)\n",
    "       # adding the new One Hot Encoded varibales to the train data frame\n",
    "       X_train_1=pd.concat([X_train_1,temp],axis=1)\n",
    "       # fitting One Hot Encoding on test data\n",
    "       temp = enc.transform(X_test[[col]])\n",
    "       # changing it into data frame and adding column names\n",
    "       temp=pd.DataFrame(temp,columns=[(col+\"_\"+str(i)) for i in data[col]\n",
    "            .value_counts().index])\n",
    "       # Setting the index for proper concatenation\n",
    "       temp=temp.set_index(X_test.index.values)\n",
    "       # adding the new One Hot Encoded varibales to test data frame\n",
    "       X_test_1=pd.concat([X_test_1,temp],axis=1)\n",
    "       \n",
    "There are some cases where LabelEncoder or DictVectorizor are useful, but these are quite limited in my opinion due to ordinality.\n",
    "LabelEncoder can turn [dog,cat,dog,mouse,cat] into [1,2,1,3,2], but then the imposed ordinality means that the average of dog and mouse is cat. Still there are algorithms like decision trees and random forests that can work with categorical variables just fine and LabelEncoder can be used to store values using less disk space.\n",
    "One-Hot-Encoding has a the advantage that the result is binary rather than ordinal and that everything sits in an orthogonal vector space. The disadvantage is that for high cardinality, the feature space can really blow up quickly and you start fighting with the curse of dimensionality. In these cases, I typically employ one-hot-encoding followed by PCA for dimensionality reduction. I find that the judicious combination of one-hot plus PCA can seldom be beat by other encoding schemes. PCA finds the linear overlap, so will naturally tend to group similar features into the same feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Feature3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering : Standardize\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "Standardization is a useful technique to transform attributes with a Gaussian distribution and differing means and standard deviations to a standard Gaussian distribution with a mean of 0 and a standard deviation of 1.\n",
    "It is most suitable for techniques that assume a Gaussian distribution in the input variables and work better with rescaled data, such as linear regression, logistic regression and linear discriminate analysis.Elements such as l1 ,l2 regularizer in linear models (logistic comes under this category) and RBF kernel in SVM in objective function of learners assumes that all the features are centered around zero and have variance in the same order.<br>\n",
    "<br>\n",
    "#Standardize data (0 mean, 1 stdev)<br>\n",
    "from sklearn.preprocessing import StandardScaler<br>\n",
    "scaler = StandardScaler().fit(X)<br>\n",
    "rescaledX = scaler.transform(X)<br>\n",
    "#summarize transformed data<br>\n",
    "numpy.set_printoptions(precision=3)<br>\n",
    "print(rescaledX[0:5,:])<br>\n",
    "Standardizing the data when using a estimator having l1 or l2 regularization helps us to increase the accuracy of the prediction model. Other learners like kNN with euclidean distance measure, k-means, SVM, perceptron, neural networks, linear discriminant analysis, principal component analysis may perform better with standardized data.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Feature4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering : Normalize\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "Normalizing in scikit-learn refers to rescaling each observation (row) to have a length of 1 (called a unit norm in linear algebra).\n",
    "This preprocessing can be useful for sparse datasets (lots of zeros) with attributes of varying scales when using algorithms that weight input values such as neural networks and algorithms that use distance measures such as K-Nearest Neighbors.<br>\n",
    "#Normalize data (length of 1<br>\n",
    "from sklearn.preprocessing import Normalizer<br>\n",
    "scaler = Normalizer().fit(X)<br>\n",
    "normalizedX = scaler.transform(X)<br>\n",
    "#summarize transformed data<br>\n",
    "numpy.set_printoptions(precision=3)<br>\n",
    "print(normalizedX[0:5,:])<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Feature5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering : Make Binary\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "You can transform your data using a binary threshold. All values above the threshold are marked 1 and all equal to or below are marked as 0. This is called binarizing your data or threshold your data. It can be useful when you have probabilities that you want to make crisp values. It is also useful when feature engineering and you want to add new features that indicate something meaningful. <br>\n",
    "<br>\n",
    "from sklearn.preprocessing import Binarizer<br>\n",
    "binarizer = Binarizer(threshold=0.0).fit(X)<br>\n",
    "binaryX = binarizer.transform(X)<br>\n",
    "#summarize transformed data<br>\n",
    "numpy.set_printoptions(precision=3)<br>\n",
    "print(binaryX[0:5,:])<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Feature6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering : Scale\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "When your data is comprised of attributes with varying scales, many machine learning algorithms can benefit from rescaling the attributes to all have the same scale.\n",
    "Often this is referred to as normalization and attributes are often rescaled into the range between 0 and 1. This is useful for optimization algorithms in used in the core of machine learning algorithms like gradient descent. It is also useful for algorithms that weight inputs like regression and neural networks and algorithms that use distance measures like K-Nearest Neighbors.<br>\n",
    "<br>\n",
    "#Rescale data (between 0 and 1)<br>\n",
    "from sklearn.preprocessing import MinMaxScaler<br>\n",
    "array = data.values<br>\n",
    "#separate array into input and output components<br>\n",
    "X = array[:,0:8]<br>\n",
    "Y = array[:,8]<br>\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))<br>\n",
    "rescaledX = scaler.fit_transform(X)<br>\n",
    "#summarize transformed data<br>\n",
    "numpy.set_printoptions(precision=3)<br>\n",
    "print(rescaledX[0:5,:])<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Feature7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering : Bin\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "Binning is a term used to indicate creating quantiles. This allows you to create an ordered, categorical variable out of a range of values. In algorithms that respond effectively use categorical information this can be useful (probably not so great for linear regression).<br>\n",
    "<br>\n",
    "#Divide all fares into quartiles<br>\n",
    "df['Fare_bin'] = pd.qcut(df['Fare'], 4)<br> \n",
    "#qcut() creates a new variable that identifies the quartile range, but we can't use the string so either<br>\n",
    "#factorize or create dummies from the result<br>\n",
    "df['Fare_bin_id'] = pd.factorize(df['Fare_bin']) <br>\n",
    "df = pd.concat([df, pd.get_dummies(df['Fare_bin']).rename(columns=lambda x: 'Fare_' + str(x))], axis=1)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Feature8'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering : Correlation and Interaction variables\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "Interaction variables capture effects of the relationship between variables. They are constructed by performing mathematical operations on sets of features. The simple approach that we use in this example is to perform basic operators (add, subtract, multiply, divide) on each pair of numerical features. We could also get much more involved and include more than 2 features in each calculation, and/or use other operators (sqrt, ln, trig functions, etc).<br>\n",
    "This process of automated feature generation can quickly produce a LOT of new variables. In our case, we use 9 features to generate 176 new interaction features. In a larger data set with dozens or hundreds of numeric features, this process can generate an overwhelming number of new interactions. Some types of models are really good at handling a very large number of features (I’ve heard of thousands to millions), which would be necessary in such a case.\n",
    "It’s very likely that some of the new interaction variables are going to be highly correlated with one of their original variables, or with other interactions, which can be a problem especially for linear models. Highly correlated variables can cause an issue called “multicollinearity”. There is a lot of information out there about how to identify, deal with, and safely ignore multicollinearity in a data set so I’ll avoid an explanation here, but I’ve included some great links at the bottom of this post if you’re interested.<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Feature9'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering : Testing\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "we can similarly set up unit tests to verify our expectations about a data set.\n",
    "We can quickly test our data using assert statements: We assert that something must be true, and if it is, then nothing happens and the notebook continues running. However, if our assertion is wrong, then the notebook stops running and brings it to our attention. For example:.<br>\n",
    "#We know that we should only have three classes\n",
    "assert len(iris_data_clean['class'].unique()) == 3\n",
    "#We know that sepal lengths for 'Iris-versicolor' should never be below 2.5 cm\n",
    "assert iris_data_clean.loc[iris_data_clean['class'] == 'Iris-versicolor', 'sepal_length_cm'].min() >= 2.5\n",
    "#We know that our data set should have no missing measurements\n",
    "assert len(iris_data_clean.loc[(iris_data_clean['sepal_length_cm'].isnull()) |\n",
    "                               (iris_data_clean['sepal_width_cm'].isnull()) |\n",
    "                               (iris_data_clean['petal_length_cm'].isnull()) |\n",
    "                               (iris_data_clean['petal_width_cm'].isnull())]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Feature10'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering : Repeat EDA \n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Feature11'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering : Takeaway\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Feature-Selection'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "The data features that you use to train your machine learning models have a huge influence on the performance you can achieve.<br>\n",
    "Irrelevant or partially relevant features can negatively impact model performance.<br>\n",
    "There are automatic feature selection techniques that you can use to prepare your machine learning data in python with scikit-learn.<br>\n",
    "Feature selection is a process where you automatically select those features in your data that contribute most to the prediction variable or output in which you are interested. Having irrelevant features in your data can decrease the accuracy of many models, especially linear algorithms like linear and logistic regression.<br>\n",
    "Three benefits of performing feature selection before modeling your data are:<br>\n",
    "Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.<br>\n",
    "Improves Accuracy: Less misleading data means modeling accuracy improves.<br>\n",
    "Reduces Training Time: Less data means that algorithms train faster.<br>\n",
    "<br>\n",
    "##### 1. Univariate Selection<br>\n",
    "Statistical tests can be used to select those features that have the strongest relationship with the output variable.\n",
    "The scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features.<br>\n",
    "The example below uses the chi squared (chi^2) statistical test for non-negative features to select 4 of the best features from the Pima Indians onset of diabetes dataset.<br>\n",
    "<br>\n",
    "#Feature Extraction with Univariate Statistical Tests (Chi-squared for classification)<br>\n",
    "from sklearn.feature_selection import SelectKBest<br>\n",
    "from sklearn.feature_selection import chi2<br>\n",
    "array = data.values<br>\n",
    "X = array[:,0:8]<br>\n",
    "Y = array[:,8]<br>\n",
    "#feature extraction<br>\n",
    "test = SelectKBest(score_func=chi2, k=4)<br>\n",
    "fit = test.fit(X, Y)<br>\n",
    "#summarize scores<br>\n",
    "numpy.set_printoptions(precision=3)<br>\n",
    "print(fit.scores_)<br>\n",
    "features = fit.transform(X)<br>\n",
    "#summarize selected features<br>\n",
    "print(features[0:5,:])<br>\n",
    "<br>\n",
    "[  111.52   1411.887    17.605    53.108  2175.565   127.669     5.393<br>\n",
    "   181.304]<br>\n",
    "[[ 148.     0.    33.6   50. ]<br>\n",
    " [  85.     0.    26.6   31. ]<br>\n",
    " [ 183.     0.    23.3   32. ]<br>\n",
    " [  89.    94.    28.1   21. ]<br>\n",
    " [ 137.   168.    43.1   33. ]]<br>\n",
    "<br>\n",
    "You can see the scores for each attribute and the 4 attributes chosen (those with the highest scores): plas, test, mass and age.\n",
    "<br>\n",
    "##### 2. Recursive Feature Elimination\n",
    "The Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain.<br>\n",
    "It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.<br>\n",
    "You can learn more about the RFE class in the scikit-learn documentation.<br>\n",
    "The example below uses RFE with the logistic regression algorithm to select the top 3 features. The choice of algorithm does not matter too much as long as it is skillful and consistent.<br>\n",
    "<br>\n",
    "from sklearn.feature_selection import RFE<br>\n",
    "from sklearn.linear_model import LogisticRegression<br>\n",
    "array = data.values<br>\n",
    "X = array[:,0:8]<br>\n",
    "Y = array[:,8]<br>\n",
    "model = LogisticRegression()<br>\n",
    "rfe = RFE(model, 3)<br>\n",
    "fit = rfe.fit(X, Y)<br>\n",
    "print(\"Num Features: \",fit.n_features_) <br>\n",
    "print(\"Selected Features: \",fit.support_)<br>\n",
    "print(\"Feature Ranking: \",fit.ranking_)<br>\n",
    "<br>\n",
    "Num Features:  3 <br>\n",
    "Selected Features:  [ True False False False False  True  True False]<br>\n",
    "Feature Ranking:  [1 2 3 5 6 1 1 4]<br>\n",
    "<br>\n",
    "You can see that RFE chose the the top 3 features as preg, mass and pedi.<br>\n",
    "These are marked True in the support array and marked with a choice “1” in the ranking array.<br>\n",
    "<br>\n",
    "##### 3. Principal Component Analysis\n",
    "Principal Component Analysis (or PCA) uses linear algebra to transform the dataset into a compressed form.<br>\n",
    "Generally this is called a data reduction technique. A property of PCA is that you can choose the number of dimensions or principal component in the transformed result.<br>\n",
    "In the example below, we use PCA and select 3 principal components.\n",
    "<br>\n",
    "from sklearn.decomposition import PCA<br>\n",
    "X = array[:,0:8]<br>\n",
    "Y = array[:,8]<br>\n",
    "#feature extraction<br>\n",
    "pca = PCA(n_components=3)<br>\n",
    "fit = pca.fit(X)<br>\n",
    "#summarize components<br>\n",
    "print(\"Explained Variance: \",fit.explained_variance_ratio_)<br>\n",
    "print(fit.components_)<br>\n",
    "<br>\n",
    "Explained Variance:  [ 0.889  0.062  0.026]<br>\n",
    "[[ -2.022e-03   9.781e-02   1.609e-02   6.076e-02   9.931e-01   1.401e-02<br>\n",
    "    5.372e-04  -3.565e-03] <br>\n",
    " [  2.265e-02   9.722e-01   1.419e-01  -5.786e-02  -9.463e-02   4.697e-02<br>\n",
    "    8.168e-04   1.402e-01]<br>\n",
    " [ -2.246e-02   1.434e-01  -9.225e-01  -3.070e-01   2.098e-02  -1.324e-01<br>\n",
    "   -6.400e-04  -1.255e-01]]<br>\n",
    "<br>\n",
    "You can see that the transformed dataset (3 principal components) bare little resemblance to the source data.<br>\n",
    "<br>\n",
    "#### 4. Feature Importance\n",
    "Bagged decision trees like Random Forest and Extra Trees can be used to estimate the importance of features.<br>\n",
    "<br>\n",
    "from sklearn.ensemble import ExtraTreesClassifier<br>\n",
    "#feature extraction<br>\n",
    "model = ExtraTreesClassifier()<br>\n",
    "model.fit(X, Y)<br>\n",
    "print(model.feature_importances_)<br>\n",
    "<br>\n",
    "[ 0.119  0.226  0.093  0.081  0.073  0.143  0.119  0.146]<br>\n",
    "<br>\n",
    "You can see that we are given an importance score for each attribute where the larger score the more important the attribute. The scores suggest at the importance of plas, age and mass.-+ <br>\n",
    "USEFUL CODE :\n",
    "<br>\n",
    "features_list = input_df.columns.values[1::]<br>\n",
    "X = input_df.values[:, 1::]<br>\n",
    "y = input_df.values[:, 0]<br>\n",
    " <br>\n",
    "#Fit a random forest with (mostly) default parameters to determine feature importance<br>\n",
    "forest = RandomForestClassifier(oob_score=True, n_estimators=10000)<br>\n",
    "forest.fit(X, y)<br>\n",
    "feature_importance = forest.feature_importances_ <br>\n",
    " <br>\n",
    "#make importances relative to max importance<br>\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())<br>\n",
    " <br>\n",
    "#A threshold below which to drop features from the final data set. Specifically, this number represents<br>\n",
    "#the percentage of the most important feature's importance value<br>\n",
    "fi_threshold = 15<br>\n",
    " <br>\n",
    "#Get the indexes of all features over the importance threshold<br>\n",
    "important_idx = np.where(feature_importance > fi_threshold)[0]<br>\n",
    " <br>\n",
    "#Create a list of all the feature names above the importance threshold<br>\n",
    "important_features = features_list[important_idx]<br>\n",
    "print \"n\", important_features.shape[0], \"Important features(>\", fi_threshold, \"% of max importance):n\",<br> \n",
    "        important_features<br>\n",
    "<br>\n",
    "#Get the sorted indexes of important features<br>\n",
    "sorted_idx = np.argsort(feature_importance[important_idx])[::-1]<br>\n",
    "print \"nFeatures sorted by importance (DESC):n\", important_features[sorted_idx]<br>\n",
    " <br>\n",
    "#Adapted from http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html <br>\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5 <br>\n",
    "plt.subplot(1, 2, 2) <br>\n",
    "plt.barh(pos, feature_importance[important_idx][sorted_idx[::-1]], align='center') <br>\n",
    "plt.yticks(pos, important_features[sorted_idx[::-1]]) <br>\n",
    "plt.xlabel('Relative Importance') <br>\n",
    "plt.title('Variable Importance') <br>\n",
    "plt.draw()<br>\n",
    "plt.show()<br>\n",
    " <br>\n",
    "#Remove non-important features from the feature set, and reorder those remaining<br>\n",
    "X = X[:, important_idx][:, sorted_idx]<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Resampling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Evaluation With Resampling Methods\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "### Split into Train and Test Sets\n",
    "The simplest method that we can use to evaluate the performance of a machine learning algorithm is to use different training and testing datasets.<br>\n",
    "We can take our original dataset, split it into two parts. Train the algorithm on the first part, make predictions on the second part and evaluate the predictions against the expected results.<br>\n",
    "The size of the split can depend on the size and specifics of your dataset, although it is common to use 67% of the data for training and the remaining 33% for testing.<br>\n",
    "This algorithm evaluation technique is very fast. It is ideal for large datasets (millions of records) where there is strong evidence that both splits of the data are representative of the underlying problem. Because of the speed, it is useful to use this approach when the algorithm you are investigating is slow to train.<br>\n",
    "A downside of this technique is that it can have a high variance. This means that differences in the training and test dataset can result in meaningful differences in the estimate of accuracy.<br>\n",
    "In the example below we split the data Pima Indians dataset into 67%/33% split for training and test and evaluate the accuracy of a Logistic Regression model.<br>\n",
    "<br>\n",
    "#Evaluate using a train and a test set<br>\n",
    "import sklearn <br>\n",
    "from sklearn import cross_validation <br>\n",
    "from sklearn.linear_model import LogisticRegression <br>\n",
    "test_size = 0.33<br>\n",
    "seed = 7<br>\n",
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, Y, test_size=test_size, random_state=seed)<br>\n",
    "model = LogisticRegression()<br>\n",
    "model.fit(X_train, Y_train)<br>\n",
    "result = model.score(X_test, Y_test)<br>\n",
    "print(\"Accuracy: {}\".format(result*100.0))<br>\n",
    "We can see that the estimated accuracy for the model was approximately 75%. Note that in addition to specifying the size of the split, we also specify the random seed. Because the split of the data is random, we want to ensure that the results are reproducible. By specifying the random seed we ensure that we get the same random numbers each time we run the code.<br>\n",
    "This is important if we want to compare this result to the estimated accuracy of another machine learning algorithm or the same algorithm with a different configuration. To ensure the comparison was apples-for-apples, we must ensure that they are trained and tested on the same data.<br>  \n",
    "### K-fold Cross Validation\n",
    "Cross validation is an approach that you can use to estimate the performance of a machine learning algorithm with less variance than a single train-test set split.<br>\n",
    "It works by splitting the dataset into k-parts (e.g. k=5 or k=10). Each split of the data is called a fold. The algorithm is trained on k-1 folds with one held back and tested on the held back fold. This is repeated so that each fold of the dataset is given a chance to be the held back test set.<br>\n",
    "After running cross validation you end up with k different performance scores that you can summarize using a mean and a standard deviation.<br>\n",
    "The result is a more reliable estimate of the performance of the algorithm on new data given your test data. It is more accurate because the algorithm is trained and evaluated multiple times on different data.<br>\n",
    "The choice of k must allow the size of each test partition to be large enough to be a reasonable sample of the problem, whilst allowing enough repetitions of the train-test evaluation of the algorithm to provide a fair estimate of the algorithms performance on unseen data. For modest sized datasets in the thousands or tens of thousands of records, k values of 3, 5 and 10 are common.<br>\n",
    "10-fold cross validation.<br>\n",
    "num_instances = len(X)<br>\n",
    "seed = 7v\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=10, random_state=seed)<br>\n",
    "model = LogisticRegression()<br>\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold)<br>\n",
    "print(\"Accuracy: \",results.mean()*100.0, results.std()*100.0)<br>\n",
    "<br>\n",
    "You can see that we report both the mean and the standard deviation of the performance measure. When summarizing performance measures, it is a good practice to summarize the distribution of the measures, in this case assuming a Gaussian distribution of performance (a very reasonable assumption) and recording the mean and standard deviation.<br>\n",
    "### Leave One Out Cross Validation\n",
    "You can configure cross validation so that the size of the fold is 1 (k is set to the number of observations in your dataset). This variation of cross validation is called leave-one-out cross validation.\n",
    "The result is a large number of performance measures that can be summarized in an effort to give a more reasonable estimate of the accuracy of your model on unseen data. A downside is that it can be a computationally more expensive procedure than k-fold cross validation.<br>\n",
    "num_folds = 10<br>\n",
    "num_instances = len(X)<br>\n",
    "loocv = cross_validation.LeaveOneOut(num_instances)<br>\n",
    "model = LogisticRegression()<br>\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=loocv)<br>\n",
    "print(\"Accuracy: \",results.mean()*100.0, results.std()*100.0)<br>\n",
    "You can see in the standard deviation that the score has more variance than the k-fold cross validation results described above.<br>\n",
    "### Repeated Random Test-Train Splits\n",
    "Another variation on k-fold cross validation is to create a random split of the data like the train/test split described above, but repeat the process of splitting and evaluation of the algorithm multiple times, like cross validation.<br>\n",
    "This has the speed of using a train/test split and the reduction in variance in the estimated performance of k-fold cross validation. You can also repeat the process many more times as need. A down side is that repetitions may include much of the same data in the train or the test split from run to run, introducing redundancy into the evaluation.<br>\n",
    "The example below splits the data into a 67%/33% train/test split and repeats the process 10 times.<br>\n",
    "kfold = cross_validation.ShuffleSplit(num_instances, test_size=test_size, random_state=seed)<br>\n",
    "model = LogisticRegression()<br>\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold)<br>\n",
    "print(\"Accuracy: \",results.mean()*100.0, results.std()*100.0)<br>\n",
    "We can see that the distribution of the performance measure is on par with k-fold cross validation above.<br>\n",
    "### Stratified k-fold cross-validation\n",
    "Stratified *k*-fold keeps the class proportions the same across all of the folds, which is vital for maintaining a representative subset of our data set. (e.g., so we don't have 100% `Iris setosa` entries in one of the folds.<br>\n",
    "<br>\n",
    "import numpy as np<br>\n",
    "from sklearn.cross_validation import StratifiedKFold<br>\n",
    "\n",
    "def plot_cv(cv, n_samples):<br>\n",
    "    masks = []<br>\n",
    "    for train, test in cv: <br>\n",
    "        mask = np.zeros(n_samples, dtype=bool)<br>\n",
    "        mask[test] = 1<br>\n",
    "        masks.append(mask)<br>\n",
    "    <br>    \n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.imshow(masks, interpolation='none')\n",
    "    plt.ylabel('Fold')\n",
    "    plt.xlabel('Row #')\n",
    "\n",
    "plot_cv(StratifiedKFold(all_classes, n_folds=10), len(all_classes))<br>\n",
    "<br>\n",
    "##### What Techniques to Use When\n",
    "Generally k-fold cross validation is the gold-standard for evaluating the performance of a machine learning algorithm on unseen data with k set to 3, 5, or 10.<br>\n",
    "Using a train/test split is good for speed when using a slow algorithm and produces performance estimates with lower bias when using large datasets.<br>\n",
    "Techniques like leave-one-out cross validation and repeated random splits can be useful intermediates when trying to balance variance in the estimated performance, model training speed and dataset size.<br>\n",
    "The best advice is to experiment and find a technique for your problem that is fast and produces reasonable estimates of performance that you can use to make decisions. If in doubt, use 10-fold cross validation.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#metrics'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance metrics\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "### Classification Metrics\n",
    "Classification problems are perhaps the most common type of machine learning problem and as such there are a myriad of metrics that can be used to evaluate predictions for these problems.\n",
    "In this section we will review how to use the following metrics:\n",
    "###### Classification Accuracy.\n",
    "###### Logarithmic Loss.\n",
    "###### Area Under ROC Curve.\n",
    "###### Confusion Matrix.\n",
    "###### Classification Report.\n",
    "#### 1. Classification Accuracy\n",
    "Classification accuracy is the number of correct predictions made as a ratio of all predictions made.<br>\n",
    "This is the most common evaluation metric for classification problems, it is also the most misused. It is really only suitable when there are an equal number of observations in each class (which is rarely the case) and that all predictions and prediction errors are equally important, which is often not the case.<br>\n",
    "scoring = 'accuracy'\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=10, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold,scoring=scoring)\n",
    "print(\"Accuracy: \",results.mean()*100.0, results.std()*100.0)\n",
    "\n",
    "#### 2. Logarithmic Loss\n",
    "Logarithmic loss (or logloss) is a performance metric for evaluating the predictions of probabilities of membership to a given class.<br>\n",
    "The scalar probability between 0 and 1 can be seen as a measure of confidence for a prediction by an algorithm. Predictions that are correct or incorrect are rewarded or punished proportionally to the confidence of the prediction.<br>\n",
    "scoring = 'log_loss'\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold,scoring=scoring)\n",
    "print(\"Accuracy: \",results.mean(), results.std())\n",
    "\n",
    "Smaller logloss is better with 0 representing a perfect logloss. As mentioned above, the measure is inverted to be ascending when using the cross_val_score() function.\n",
    "\n",
    "##### 3. Area Under ROC Curve\n",
    "Area under ROC Curve (or AUC for short) is a performance metric for binary classification problems.\n",
    "The AUC represents a model’s ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random.<br>\n",
    "ROC can be broken down into sensitivity and specificity. A binary classification problem is really a trade-off between sensitivity and specificity.<br>\n",
    "Sensitivity is the true positive rate also called the recall. It is the number instances from the positive (first) class that actually predicted correctly.<br>\n",
    "Specificity is also called the true negative rate. Is the number of instances from the negative class (second) class that were actually predicted correctly.<br>\n",
    "An ROC curve is the most commonly used way to visualize the performance of a binary classifier, and AUC is (arguably) the best way to summarize its performance in a single number.<br>\n",
    "scoring = 'roc_auc'\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold,scoring=scoring)\n",
    "print(\"Accuracy: \",results.mean(), results.std())\n",
    "If the AUC is relatively close to 1 and greater than 0.5, suggesting some skill in the predictions\n",
    "\n",
    "#### 4. Confusion Matrix\n",
    "The confusion matrix is a handy presentation of the accuracy of a model with two or more classes.\n",
    "The table presents predictions on the x-axis and accuracy outcomes on the y-axis. The cells of the table are the number of predictions made by a machine learning algorithm.<br>\n",
    "For example, a machine learning algorithm can predict 0 or 1 and each prediction may actually have been a 0 or 1. Predictions for 0 that were actually 0 appear in the cell for prediction=0 and actual=0, whereas predictions for 0 that were actually 1 appear in the cell for prediction = 0 and actual=1. And so on.<br>\n",
    "from sklearn.metrics import confusion_matrix\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "predicted = model.predict(X_test)\n",
    "matrix = confusion_matrix(Y_test, predicted)\n",
    "print(matrix)\n",
    "\n",
    "#### 5. Classification Report\n",
    "Scikit-learn does provide a convenience report when working on classification problems to give you a quick idea of the accuracy of a model using a number of measures.<br>\n",
    "The classification_report() function displays the precision, recall, f1-score and support for each class.\n",
    "from sklearn.metrics import classification_report\n",
    "0report = classification_report(Y_test, predicted)\n",
    "print(report)\n",
    "<b>Mean F Score</b><br>\n",
    "The F1 score, commonly used in information retrieval, measures accuracy using the statistics precision p and recall r. Precision is the ratio of true positives (tp) to all predicted positives (tp + fp). Recall is the ratio of true positives to all actual positives (tp + fn). The F1 score is given by <br>\n",
    "F1= 2*(pr)/(p+r) <br>\n",
    "  where  p= (tp)/(tp + fp) , r= (tp)/(tp + fn)<br>\n",
    "<br>\n",
    "The F1 metric weights recall and precision equally, and a good retrieval algorithm will maximize both precision and recall simultaneously. Thus moderately good performance on both will be favored over extremely good performance on one and poor performance on the other.\n",
    "For MeanF1Score calculate F1 metrics for each label, and find their average, weighted by support (the number of true instances for each label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Spot-check'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spot-check Algorithms\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "Spot-checking is a way of discovering which algorithms perform well on your machine learning problem.<br>\n",
    "You cannot know which algorithms are best suited to your problem before hand. You must trial a number of methods and focus attention on those that prove themselves the most promising.<br>\n",
    "You can guess at what algorithms might do well on your dataset, and this can be a good starting point.<br>\n",
    "\n",
    "Try a mixture of algorithm representations (e.g. instances and trees).<br>\n",
    "Try a mixture of learning algorithms (e.g. different algorithms for learning the same type of representation).<br>\n",
    "Try a mixture of modeling types (e.g. linear and nonlinear functions or parametric and nonparametric).<br>\n",
    "Let’s get specific. <br>\n",
    "Algorithms Overview<br>\n",
    "We are going to take a look at 6 classification algorithms to spot check on your dataset.<br>\n",
    "##### 2 Linear Machine Learning Algorithms:\n",
    "Logistic Regression <br>\n",
    "Linear Discriminant Analysis <br>\n",
    "###### 4 Nonlinear Machine Learning Algorithms:\n",
    "K-Nearest Neighbors<br>\n",
    "Naive Bayes<br>\n",
    "Classification and Regression Trees<br>\n",
    "Support Vector Machines<br>\n",
    "<br>\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=10, random_state=seed)<br>\n",
    "model = LogisticRegression()<br>\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold)<br>\n",
    "print(\"Accuracy: \",results.mean()*100.0, results.std()*100.0)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Spot-check'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison and Selection\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "<br>\n",
    "#prepare models<br>\n",
    "models = []<br>\n",
    "models.append(('LR', LogisticRegression()))<br>\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))<br>\n",
    "models.append(('KNN', KNeighborsClassifier()))<br>\n",
    "models.append(('CART', DecisionTreeClassifier()))<br>\n",
    "models.append(('NB', GaussianNB()))<br>\n",
    "models.append(('SVM', SVC()))<br>\n",
    "#evaluate each model in turn<br>\n",
    "results = []<br>\n",
    "names = []<br>\n",
    "scoring = 'accuracy' <br>\n",
    "for name, model in models:<br>\n",
    "    kfold = cross_validation.KFold(n=num_instances, n_folds=10, random_state=seed)<br>\n",
    "    cv_results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)<br>\n",
    "    results.append(cv_results)<br>\n",
    "    names.append(name)<br>\n",
    "    print(\"{} : {} , {}\".format(name, cv_results.mean(), cv_results.std()))<br>\n",
    "#boxplot algorithm comparison<br>\n",
    "fig = plt.figure()<br>\n",
    "fig.suptitle('Algorithm Comparison')<br>\n",
    "ax = fig.add_subplot(111)<br>\n",
    "plt.boxplot(results)<br>\n",
    "ax.set_xticklabels(names)<br>\n",
    "plt.show()<br>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "========================\n",
    "\n",
    "Plotting Learning Curves\n",
    "\n",
    "========================\n",
    "\n",
    "\n",
    "\n",
    "On the left side the learning curve of a naive Bayes classifier is shown for\n",
    "\n",
    "the digits dataset. Note that the training score and the cross-validation score\n",
    "\n",
    "are both not very good at the end. However, the shape of the curve can be found\n",
    "\n",
    "in more complex datasets very often: the training score is very high at the\n",
    "\n",
    "beginning and decreases and the cross-validation score is very low at the\n",
    "\n",
    "beginning and increases. On the right side we see the learning curve of an SVM\n",
    "\n",
    "with RBF kernel. We can see clearly that the training score is still around\n",
    "\n",
    "the maximum and the validation score could be increased with more training\n",
    "\n",
    "samples.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "\n",
    "\n",
    "    Parameters\n",
    "\n",
    "    ----------\n",
    "\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "\n",
    "\n",
    "    title : string\n",
    "\n",
    "        Title for the chart.\n",
    "\n",
    "\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "\n",
    "        n_features is the number of features.\n",
    "\n",
    "\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "\n",
    "        Target relative to X for classification or regression;\n",
    "\n",
    "        None for unsupervised learning.\n",
    "\n",
    "\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "\n",
    "        Determines the cross-validation splitting strategy.\n",
    "\n",
    "        Possible inputs for cv are:\n",
    "\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "\n",
    "          - integer, to specify the number of folds.\n",
    "\n",
    "          - An object to be used as a cross-validation generator.\n",
    "\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.title(title)\n",
    "\n",
    "    if ylim is not None:\n",
    "\n",
    "        plt.ylim(*ylim)\n",
    "\n",
    "    plt.xlabel(\"Training examples\")\n",
    "\n",
    "    plt.ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.grid()\n",
    "\n",
    "\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "\n",
    "                     color=\"r\")\n",
    "\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "\n",
    "             label=\"Training score\")\n",
    "\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "    return plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "title = \"Learning Curves (Naive Bayes)\"\n",
    "\n",
    "#Cross validation with 100 iterations to get smoother mean test and train\n",
    "\n",
    "#score curves, each time with 20% data randomly selected as a validation set.\n",
    "\n",
    "cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "estimator = GaussianNB()\n",
    "\n",
    "plot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "\n",
    "\n",
    "title = \"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n",
    "\n",
    "#SVC is more expensive so we do a lower number of CV iterations:\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "\n",
    "estimator = SVC(gamma=0.001)\n",
    "\n",
    "plot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "========================\n",
    "\n",
    "Plotting Learning Curves\n",
    "\n",
    "========================\n",
    "\n",
    "\n",
    "\n",
    "On the left side the learning curve of a naive Bayes classifier is shown for\n",
    "\n",
    "the digits dataset. Note that the training score and the cross-validation score\n",
    "\n",
    "are both not very good at the end. However, the shape of the curve can be found\n",
    "\n",
    "in more complex datasets very often: the training score is very high at the\n",
    "\n",
    "beginning and decreases and the cross-validation score is very low at the\n",
    "\n",
    "beginning and increases. On the right side we see the learning curve of an SVM\n",
    "\n",
    "with RBF kernel. We can see clearly that the training score is still around\n",
    "\n",
    "the maximum and the validation score could be increased with more training\n",
    "\n",
    "samples.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "\n",
    "\n",
    "    Parameters\n",
    "\n",
    "    ----------\n",
    "\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "\n",
    "\n",
    "    title : string\n",
    "\n",
    "        Title for the chart.\n",
    "\n",
    "\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "\n",
    "        n_features is the number of features.\n",
    "\n",
    "\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "\n",
    "        Target relative to X for classification or regression;\n",
    "\n",
    "        None for unsupervised learning.\n",
    "\n",
    "\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "\n",
    "        Determines the cross-validation splitting strategy.\n",
    "\n",
    "        Possible inputs for cv are:\n",
    "\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "\n",
    "          - integer, to specify the number of folds.\n",
    "\n",
    "          - An object to be used as a cross-validation generator.\n",
    "\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.title(title)\n",
    "\n",
    "    if ylim is not None:\n",
    "\n",
    "        plt.ylim(*ylim)\n",
    "\n",
    "    plt.xlabel(\"Training examples\")\n",
    "\n",
    "    plt.ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.grid()\n",
    "\n",
    "\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "\n",
    "                     color=\"r\")\n",
    "\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "\n",
    "             label=\"Training score\")\n",
    "\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "    return plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "title = \"Learning Curves (Naive Bayes)\"\n",
    "\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "\n",
    "cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "estimator = GaussianNB()\n",
    "\n",
    "plot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "\n",
    "\n",
    "title = \"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n",
    "\n",
    "# SVC is more expensive so we do a lower number of CV iterations:\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "\n",
    "estimator = SVC(gamma=0.001)\n",
    "\n",
    "plot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Adapted from http://scikit-learn.org/stable/auto_examples/plot_learning_curve.html\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.learning_curve import learning_curve\n",
    "\n",
    "# assume classifier and training data is prepared...\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "        forest, X, y, cv=10, n_jobs=-1, train_sizes=np.linspace(.1, 1., 10), verbose=0)\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"RandomForestClassifier\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim((0.6, 1.01))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid()\n",
    "\n",
    "# Plot the average training and test score lines at each training set size\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\", label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"r\", label=\"Test score\")\n",
    "\n",
    "# Plot the std deviation as a transparent range at each training set size\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, \n",
    "                 alpha=0.1, color=\"b\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, \n",
    "                 alpha=0.1, color=\"r\")\n",
    "\n",
    "# Draw the plot and reset the y-axis\n",
    "plt.draw()\n",
    "plt.show()\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Hyperparameters'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter optimisation\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "Phrased as a search problem, you can use different search strategies to find a good and robust parameter or set of parameters for an algorithm on a given problem.<br>\n",
    "Two simple and easy search strategies are grid search and random search. Scikit-learn provides these two methods for algorithm parameter tuning and examples of each are provided below.<br>\n",
    "#### Grid Search Parameter Tuning\n",
    "Grid search is an approach to parameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid. This is a one-dimensional grid search.<br>\n",
    "<br>\n",
    "from sklearn.linear_model import Ridge<br>\n",
    "from sklearn.grid_search import GridSearchCV<br>\n",
    "#prepare a range of alpha values to test<br>\n",
    "alphas = numpy.array([1,0.1,0.01,0.001,0.0001,0])<br>\n",
    "#create and fit a ridge regression model, testing each alpha <br>\n",
    "model = Ridge()<br>\n",
    "grid = GridSearchCV(estimator=model, param_grid=dict(alpha=alphas))<br>\n",
    "grid.fit(X,Y)<br>\n",
    "print(grid)<br>\n",
    "#summarize the results of the grid search<br>\n",
    "print(grid.best_score_)<br>\n",
    "print(grid.best_estimator_.alpha)<br>\n",
    "##### Random Search Parameter Tuning\n",
    "Random search is an approach to parameter tuning that will sample algorithm parameters from a random distribution (i.e. uniform) for a fixed number of iterations. A model is constructed and evaluated for each combination of parameters chosen.<br>\n",
    "<br>\n",
    "from scipy.stats import uniform as sp_rand<br>\n",
    "from sklearn.linear_model import Ridge<br>\n",
    "from sklearn.grid_search import RandomizedSearchCV<br>\n",
    "#prepare a uniform distribution to sample for the alpha parameter<br>\n",
    "param_grid = {'alpha': sp_rand()}<br>\n",
    "#create and fit a ridge regression model, testing random alpha values<br>\n",
    "model = Ridge()<br>\n",
    "rsearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100) <br>\n",
    "rsearch.fit(X,Y) <br>\n",
    "print(rsearch) <br>\n",
    "#summarize the results of the random parameter search <br>\n",
    "print(rsearch.best_score_) <br>\n",
    "print(rsearch.best_estimator_.alpha) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "parameter_grid = {'max_depth': [1, 2, 3, 4, 5],\n",
    "                  'max_features': [1, 2, 3, 4]}\n",
    "\n",
    "cross_validation = StratifiedKFold(all_classes, n_folds=10)\n",
    "\n",
    "grid_search = GridSearchCV(decision_tree_classifier,\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=cross_validation)\n",
    "\n",
    "grid_search.fit(all_inputs, all_classes)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_visualization = []\n",
    "\n",
    "for grid_pair in grid_search.grid_scores_:\n",
    "    grid_visualization.append(grid_pair.mean_validation_score)\n",
    "    \n",
    "grid_visualization = np.array(grid_visualization)\n",
    "grid_visualization.shape = (5, 4)\n",
    "sb.heatmap(grid_visualization, cmap='Blues')\n",
    "plt.xticks(np.arange(4) + 0.5, grid_search.param_grid['max_features'])\n",
    "plt.yticks(np.arange(5) + 0.5, grid_search.param_grid['max_depth'][::-1])\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('max_depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function to report optimal parameters\n",
    "# (adapted from http://scikit-learn.org/stable/auto_examples/randomized_search.html)\n",
    "def report(grid_scores, n_top=5):\n",
    "    params = None\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Parameters with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.4f} (std: {1:.4f})\".format(\n",
    "              score.mean_validation_score, np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")\n",
    "        \n",
    "        if params == None:\n",
    "            params = score.parameters\n",
    "    \n",
    "    return params\n",
    "\n",
    "# The most common value for the max number of features to look at in each split is sqrt(# of features)\n",
    "sqrtfeat = np.sqrt(X.shape[1]) \n",
    "\n",
    "# Simple grid test (162 combinations)\n",
    "grid_test1 = { \"n_estimators\"      : [1000, 2500, 5000],\n",
    "               \"criterion\"         : [\"gini\", \"entropy\"],\n",
    "               \"max_features\"      : [sqrtfeat-1, sqrtfeat, sqrtfeat+1],\n",
    "               \"max_depth\"         : [5, 10, 25],\n",
    "               \"min_samples_split\" : [2, 5, 10] }\n",
    "\n",
    "# Large randomized test using max_depth to control tree size (5000 possible combinations)\n",
    "random_test1 = { \"n_estimators\"      : np.rint(np.linspace(X.shape[0]*2, X.shape[0]*4, 5)).astype(int),\n",
    "                 \"criterion\"         : [\"gini\", \"entropy\"],\n",
    "                 \"max_features\"      : np.rint(np.linspace(sqrtfeat/2, sqrtfeat*2, 5)).astype(int),\n",
    "                 \"max_depth\"         : np.rint(np.linspace(1, X.shape[1]/2, 10),\n",
    "                 \"min_samples_split\" : np.rint(np.linspace(2, X.shape[0]/50, 10)).astype(int) }\n",
    "\n",
    "# Large randomized test using min_samples_leaf and max_leaf_nodes to control tree size (50k combinations)\n",
    "random_test2 = { \"n_estimators\"      : np.rint(np.linspace(X.shape[0]*2, X.shape[0]*4, 5)).astype(int),\n",
    "                 \"criterion\"         : [\"gini\", \"entropy\"],\n",
    "                 \"max_features\"      : np.rint(np.linspace(sqrtfeat/2, sqrtfeat*2, 5)).astype(int),\n",
    "                 \"min_samples_split\" : np.rint(np.linspace(2, X.shape[0]/50, 10)).astype(int),\n",
    "                 \"min_samples_leaf\"  : np.rint(np.linspace(1, X.shape[0]/200, 10)).astype(int), \n",
    "                 \"max_leaf_nodes\"    : np.rint(np.linspace(10, X.shape[0]/50, 10)).astype(int) }\n",
    "\n",
    "forest = RandomForestClassifier(oob_score=True)\n",
    "\n",
    "print \"Hyperparameter optimization using GridSearchCV...\"\n",
    "grid_search = GridSearchCV(forest, grid_test1, n_jobs=-1, cv=10)\n",
    "grid_search.fit(X, y)\n",
    "best_params_from_grid_search = scorereport.report(grid_search.grid_scores_)\n",
    "\n",
    "print \"Hyperparameter optimization using RandomizedSearchCV with max_depth parameter...\"\n",
    "grid_search = RandomizedSearchCV(forest, random_test1, n_jobs=-1, cv=10, n_iter=100)\n",
    "grid_search.fit(X, y)\n",
    "best_params_from_rand_search1 = scorereport.report(grid_search.grid_scores_)\n",
    "\n",
    "print \"...and using RandomizedSearchCV with min_samples_leaf + max_leaf_nodes parameters...\"\n",
    "grid_search = RandomizedSearchCV(forest, random_test2, n_jobs=-1, cv=10, n_iter=500)\n",
    "grid_search.fit(X, y)\n",
    "best_params_from_rand_search2 = scorereport.report(grid_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Pipeline'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A single Pipeline\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# We can jump directly to working with the clean data because we saved our cleaned data set\n",
    "iris_data_clean = pd.read_csv('iris-data-clean.csv')\n",
    "\n",
    "# Testing our data: Our analysis will stop here if any of these assertions are wrong\n",
    "\n",
    "# We know that we should only have three classes\n",
    "assert len(iris_data_clean['class'].unique()) == 3\n",
    "\n",
    "# We know that sepal lengths for 'Iris-versicolor' should never be below 2.5 cm\n",
    "assert iris_data_clean.loc[iris_data_clean['class'] == 'Iris-versicolor', 'sepal_length_cm'].min() >= 2.5\n",
    "\n",
    "# We know that our data set should have no missing measurements\n",
    "assert len(iris_data_clean.loc[(iris_data_clean['sepal_length_cm'].isnull()) |\n",
    "                               (iris_data_clean['sepal_width_cm'].isnull()) |\n",
    "                               (iris_data_clean['petal_length_cm'].isnull()) |\n",
    "                               (iris_data_clean['petal_width_cm'].isnull())]) == 0\n",
    "\n",
    "all_inputs = iris_data_clean[['sepal_length_cm', 'sepal_width_cm',\n",
    "                             'petal_length_cm', 'petal_width_cm']].values\n",
    "\n",
    "all_classes = iris_data_clean['class'].values\n",
    "\n",
    "# This is the classifier that came out of Grid Search\n",
    "random_forest_classifier = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                                max_depth=None, max_features=3, max_leaf_nodes=None,\n",
    "                                min_samples_leaf=1, min_samples_split=2,\n",
    "                                min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=1,\n",
    "                                oob_score=False, random_state=None, verbose=0, warm_start=True)\n",
    "\n",
    "# All that's left to do now is plot the cross-validation scores\n",
    "rf_classifier_scores = cross_val_score(random_forest_classifier, all_inputs, all_classes, cv=10)\n",
    "sb.boxplot(rf_classifier_scores)\n",
    "sb.stripplot(rf_classifier_scores, jitter=True, color='white')\n",
    "\n",
    "# ...and show some of the predictions from the classifier\n",
    "(training_inputs,\n",
    " testing_inputs,\n",
    " training_classes,\n",
    " testing_classes) = train_test_split(all_inputs, all_classes, train_size=0.75)\n",
    "\n",
    "random_forest_classifier.fit(training_inputs, training_classes)\n",
    "\n",
    "for input_features, prediction, actual in zip(testing_inputs[:10],\n",
    "                                              random_forest_classifier.predict(testing_inputs[:10]),\n",
    "                                              testing_classes[:10]):\n",
    "    print('{}\\t-->\\t{}\\t(Actual: {})'.format(input_features, prediction, actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Pickle'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalize the model\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "##### Finalize Your Model with pickle\n",
    "Pickle is the standard way of serializing objects in Python.\n",
    "You can use the pickle operation to serialize your machine learning algorithms and save the serialized format to a file.\n",
    "Later you can load this file to deserialize your model and use it to make new predictions.\n",
    "<br>\n",
    "#Running the example saves the model to finalized_model.sav in your local working directory<br>\n",
    "import pickle<br>\n",
    "test_size = 0.33<br>\n",
    "seed = 7<br>\n",
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, Y, test_size=test_size, random_state=seed)<br>\n",
    "#Fit the model on 33%<br>\n",
    "model = LogisticRegression()<br>\n",
    "model.fit(X_train, Y_train)<br>\n",
    "#save the model to disk<br>\n",
    "filename = 'finalized_model.sav'<br>\n",
    "pickle.dump(model, open(filename, 'wb'))<br>\n",
    "<br>\n",
    "#load the model from disk<br>\n",
    "loaded_model = pickle.load(open(filename, 'rb'))<br>\n",
    "result = loaded_model.score(X_test, Y_test)<br>\n",
    "print(result)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#time'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If I had more time ....\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Acknowledgement'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgements\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "<br>\n",
    "<br>\n",
    "http://machinelearningmastery.com<br>\n",
    "Notebook by [Randal S. Olson](http://www.randalolson.com/)<br>\n",
    "http://www.ultravioletanalytics.com<br>\n",
    "https://www.analyticsvidhya.com<br>\n",
    "https://sebastianraschka.com/<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
